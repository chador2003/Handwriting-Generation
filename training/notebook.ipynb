{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a5b2f7",
   "metadata": {
    "id": "8pXzMXHqFe_m",
    "papermill": {
     "duration": 0.856248,
     "end_time": "2023-12-01T12:16:00.886837",
     "exception": false,
     "start_time": "2023-12-01T12:16:00.030589",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978c540a",
   "metadata": {
    "id": "GOdkkjRfwd4H",
    "papermill": {
     "duration": 0.007721,
     "end_time": "2023-12-01T12:16:00.903025",
     "exception": false,
     "start_time": "2023-12-01T12:16:00.895304",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Dataset\n",
    "We run through the following steps:\n",
    "1. Import the dataset from Kaggle and load into a `pd.DataFrame`\n",
    "2. Extract the images and labels from the `pd.DataFrame`\n",
    "\n",
    "We will use the encoding that:\n",
    "- The digits \"0\"->\"9\" map to 0->9\n",
    "- The alphabet \"A\"->\"Z\" map to 10->35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1ec184",
   "metadata": {
    "id": "4RSnaWKlwd4I",
    "papermill": {
     "duration": 35.648326,
     "end_time": "2023-12-01T12:16:36.559242",
     "exception": false,
     "start_time": "2023-12-01T12:16:00.910916",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./data/A_Z Handwritten Data.csv\", dtype=\"float32\")\n",
    "X = data.drop(columns=\"0\").astype(\"float32\").divide(255)\n",
    "y = data[\"0\"].astype(\"int64\").add(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b6e5ac",
   "metadata": {
    "id": "e86ZmnDoB3jl",
    "papermill": {
     "duration": 0.015244,
     "end_time": "2023-12-01T12:16:36.582778",
     "exception": false,
     "start_time": "2023-12-01T12:16:36.567534",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def label_to_string(y: int) -> str:\n",
    "    if y < 10:\n",
    "        return str(y)\n",
    "    return chr(y - 10 + ord(\"A\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2266f415",
   "metadata": {
    "id": "Lxobu46Uwd4J",
    "papermill": {
     "duration": 5.474505,
     "end_time": "2023-12-01T12:16:44.522219",
     "exception": false,
     "start_time": "2023-12-01T12:16:39.047714",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torch.utils.data import ConcatDataset, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "class HandwritingDataset(Dataset):\n",
    "    def __init__(self, X: pd.DataFrame, y: pd.Series, transform=None, target_transform=None):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.X.iloc[idx, :].values.reshape(28, 28)\n",
    "        label = self.y.iloc[idx]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return image, label\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "transform = torchvision.transforms.ToTensor()\n",
    "train_dataset = ConcatDataset(\n",
    "    [\n",
    "        HandwritingDataset(X_train, y_train, transform=transform),\n",
    "        torchvision.datasets.MNIST(root='./data/mnist', train=True, download=True, transform=transform),\n",
    "    ]\n",
    ")\n",
    "test_dataset = ConcatDataset(\n",
    "    [\n",
    "        HandwritingDataset(X_test, y_test, transform=transform),\n",
    "        torchvision.datasets.MNIST(root='./data/mnist', train=False, download=True, transform=transform),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a5d50b",
   "metadata": {
    "id": "di6weTSlMpgA",
    "papermill": {
     "duration": 0.111807,
     "end_time": "2023-12-01T12:16:44.643807",
     "exception": false,
     "start_time": "2023-12-01T12:16:44.532000",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ffb598",
   "metadata": {
    "id": "WzAknh7hwd4K",
    "papermill": {
     "duration": 0.016785,
     "end_time": "2023-12-01T12:16:44.670032",
     "exception": false,
     "start_time": "2023-12-01T12:16:44.653247",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# put into batches\n",
    "batch_size = 64\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca75a14a",
   "metadata": {
    "papermill": {
     "duration": 0.009059,
     "end_time": "2023-12-01T12:16:44.767622",
     "exception": false,
     "start_time": "2023-12-01T12:16:44.758563",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Training a model\n",
    "\n",
    "## Model definition\n",
    "We will use a supervised-VAE with the following architecture.\n",
    "\n",
    "### Encoder\n",
    "This part of the model predicts the class labels and latent variables from a given image.\n",
    "- Convolutional layer + ReLU (input size 28x28x1, output size 14x14x`num_filters`)\n",
    "- Convolutional layer + ReLU (input size 14x14x`num_filters`, output size 7x7x`num_filters`)\n",
    "- Flatten (input size 7x7x`num_filters`, output size 49x`num_filters`)\n",
    "- To generate *class labels* `y`:\n",
    "    - Linear + ReLU (input size 49x`num_filters`, output size 128)\n",
    "    - Linear (input size 128, output size `output_size`)\n",
    "    - **Note:** there is no `Softmax` here since this is implicitly included as part of the `CrossEntropyLoss` used later on\n",
    "- To predict mean and variance of *latent variables* `z`:\n",
    "    - Linear (input size 49x`num_filters`, output size `num_latent_var`)\n",
    "\n",
    "### Decoder\n",
    "This part of the model reconstructs an image from class labels and latent variables.\n",
    "- From *class labels* `y` logits:\n",
    "    - Softmax (to convert from logits to class probabilities)\n",
    "    - Linear + ReLU (input size `num_latent_var`, output size 128)\n",
    "    - Linear (input size 128, output size 49x`num_filters`)\n",
    "- From *latent variables* `z`:\n",
    "    - Linear (input size `num_latent_var`, output size 49x`num_filters`)\n",
    "- Unflatten (input size 49x`num_filters`, output size 7x7x`num_filters`,)\n",
    "- Deconvolutional layer + ReLU (input size 7x7x`num_filters`, output size 14x14x`num_filters`)\n",
    "- Deconvolutional layer (input size 14x14x`num_filters`, output size 28x28x1)\n",
    "- **Note:** there is no `Sigmoid` here since this is implicitly included as part of the `BCEWithLogitsLoss` used later on\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a906ca",
   "metadata": {
    "id": "1nnmb13qPPSf",
    "papermill": {
     "duration": 3.06753,
     "end_time": "2023-12-01T12:16:47.844665",
     "exception": false,
     "start_time": "2023-12-01T12:16:44.777135",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from training.model import VAE\n",
    "\n",
    "model = VAE(input_size=28, output_size=10 + 26, num_filters=32, num_latent_var=64).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50382708",
   "metadata": {},
   "source": [
    "## Loss\n",
    "For a supervised VAE, we need 3 loss terms:\n",
    "1. **Reconstruction loss** to enforce that the decoder can accuractely reconstruct characters from the latent variables (using `BCEWithLogitsLoss`)\n",
    "2. **KL-loss** to enforce that the encoder accuractely predicts the posterior on the latent variables\n",
    "3. **Categorical loss** to enforce that the model can accuractely classify characters (using `CrossEntropyLoss`)\n",
    "\n",
    "1 & 2 would be needed for an unsupervised VAE as well, but 3 is an additional loss needed for the supervised nature of this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e66c65",
   "metadata": {
    "papermill": {
     "duration": 0.019221,
     "end_time": "2023-12-01T12:16:47.894388",
     "exception": false,
     "start_time": "2023-12-01T12:16:47.875167",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "cat_loss_fun = nn.CrossEntropyLoss()\n",
    "recon_loss_fun = nn.BCEWithLogitsLoss()\n",
    "\n",
    "\n",
    "def kl_div_loss_fun(z_mean: torch.Tensor, log_z_var: torch.Tensor) -> torch.Tensor:\n",
    "    return -0.5 * torch.sum(1 + log_z_var - z_mean.pow(2) - log_z_var.exp()) / z_mean.shape[0]\n",
    "\n",
    "\n",
    "def loss_fun(model: VAE, x: torch.Tensor, y: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "    x_recon, z_mean, log_z_var, y_pred = model.forward(x)\n",
    "    return cat_loss_fun(y_pred, y), recon_loss_fun(x_recon, x), kl_div_loss_fun(z_mean, log_z_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d89c54c",
   "metadata": {
    "id": "S3TZJjYCeYX5",
    "outputId": "90b979b9-29a1-4cbb-dca0-c01a05375edc",
    "papermill": {
     "duration": 769.131049,
     "end_time": "2023-12-01T12:29:37.034706",
     "exception": false,
     "start_time": "2023-12-01T12:16:47.903657",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "optimiser = torch.optim.Adam(model.parameters())\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    minloss = 1\n",
    "    running_kl_div_loss = 0\n",
    "    running_recons_loss = 0\n",
    "    running_cat_loss = 0\n",
    "    num_images = 0\n",
    "    for i, (img, label) in enumerate(train_loader):\n",
    "        img = img.to(device)\n",
    "        label = label.to(device)\n",
    "        optimiser.zero_grad()\n",
    "        cat_loss, recons_loss, kl_div_loss = loss_fun(model, img, label)\n",
    "        loss = 0.1 * cat_loss + recons_loss + epoch * 0.001 * kl_div_loss\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "        running_cat_loss = running_cat_loss + cat_loss.item() * len(img)\n",
    "        running_recons_loss = running_recons_loss + recons_loss.item() * len(img)\n",
    "        running_kl_div_loss = running_kl_div_loss + kl_div_loss.item() * len(img)\n",
    "\n",
    "        num_images = num_images + len(img)\n",
    "    print(\n",
    "        'epoch: '\n",
    "        + str(epoch)\n",
    "        + ' cat_loss: '\n",
    "        + str(running_cat_loss / num_images)\n",
    "        + ' recons_loss: '\n",
    "        + str(running_recons_loss / num_images)\n",
    "        + ' kl_div_loss: '\n",
    "        + str(running_kl_div_loss / num_images)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8d73b3",
   "metadata": {
    "papermill": {
     "duration": 0.010134,
     "end_time": "2023-12-01T12:29:37.055311",
     "exception": false,
     "start_time": "2023-12-01T12:29:37.045177",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Save model to file\n",
    "We need to save the model so that we can use it for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92ca0d3",
   "metadata": {
    "papermill": {
     "duration": 0.028411,
     "end_time": "2023-12-01T12:29:37.094455",
     "exception": false,
     "start_time": "2023-12-01T12:29:37.066044",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"model.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f0bdc5",
   "metadata": {
    "id": "eJpLnBvEwd4L",
    "papermill": {
     "duration": 0.053148,
     "end_time": "2023-12-01T12:29:37.157930",
     "exception": false,
     "start_time": "2023-12-01T12:29:37.104782",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Assessing model accuracy\n",
    "We must first evaluate the model on the test set.\n",
    "\n",
    "We then can assess the accuracy in 2 ways:\n",
    "- Compute the multi-class classifiction accuracy\n",
    "- Compute the binary accuracy of the reconstructed images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53cf3ce6",
   "metadata": {
    "id": "Rn4bYw3hmkEW",
    "papermill": {
     "duration": 14.145929,
     "end_time": "2023-12-01T12:29:51.314336",
     "exception": false,
     "start_time": "2023-12-01T12:29:37.168407",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "x_truths = []\n",
    "x_recons = []\n",
    "z_means = []\n",
    "log_z_vars = []\n",
    "y_truths = []\n",
    "y_preds = []\n",
    "\n",
    "for im, y_true in test_loader:\n",
    "    x_recon, z_mean, log_z_var, ysoft = model.forward(im.to(device))\n",
    "    im_recon = x_recon.sigmoid().detach()\n",
    "    _, y_pred = torch.max(ysoft, 1)\n",
    "    x_truths.append(im)\n",
    "    x_recons.append(im_recon.cpu())\n",
    "    z_means.append(z_mean.cpu())\n",
    "    log_z_vars.append(log_z_var.cpu())\n",
    "    y_truths.append(y_true.cpu())\n",
    "    y_preds.append(y_pred.cpu())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f775c2",
   "metadata": {
    "papermill": {
     "duration": 0.01045,
     "end_time": "2023-12-01T12:29:51.335221",
     "exception": false,
     "start_time": "2023-12-01T12:29:51.324771",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Reconstruction accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86649768",
   "metadata": {
    "papermill": {
     "duration": 5.58042,
     "end_time": "2023-12-01T12:29:56.926358",
     "exception": false,
     "start_time": "2023-12-01T12:29:51.345938",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torchmetrics.classification import BinaryAccuracy\n",
    "\n",
    "recon_metric = BinaryAccuracy(threshold=0.5)\n",
    "reconstructed_images = torch.cat(x_recons)\n",
    "original_images = torch.cat(x_truths)\n",
    "print(\"Recon. acc: {}\".format(recon_metric(reconstructed_images, original_images > 0.5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c25790a",
   "metadata": {
    "papermill": {
     "duration": 0.010391,
     "end_time": "2023-12-01T12:29:56.947711",
     "exception": false,
     "start_time": "2023-12-01T12:29:56.937320",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We can also plot the accuracy over the image. We can see that the outer black pixels are clearly well predicted, but the white pixels with the character is located are less accurately predicted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd872e0",
   "metadata": {
    "papermill": {
     "duration": 2.670627,
     "end_time": "2023-12-01T12:29:59.628783",
     "exception": true,
     "start_time": "2023-12-01T12:29:56.958156",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "recon_metric_image = np.array(\n",
    "    [\n",
    "        [recon_metric(reconstructed_images[:, :, i, j], original_images[:, :, i, j] > 0.5) for j in range(28)]\n",
    "        for i in range(28)\n",
    "    ]\n",
    ")\n",
    "plt.imshow(recon_metric_image)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50b27b9",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### Categorisation accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6fd034f",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "We can evaluate the total accuracy, as well as the accuracy on each character. We can see that all characters are very accurately classified apart from \"0\". This is likely because this can be each miscategorised with many different characters such as \"O\" or \"D\" when poorly written."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906e257b",
   "metadata": {
    "id": "Rn4bYw3hmkEW",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torchmetrics.classification import MulticlassAccuracy\n",
    "\n",
    "total_cat_metric = MulticlassAccuracy(num_classes=model.output_size, average=\"weighted\")\n",
    "cat_accuracy_total = total_cat_metric(torch.cat(y_preds), torch.cat(y_truths))\n",
    "print(\"Cat. acc (total): {}\".format(cat_accuracy_total))\n",
    "\n",
    "cat_metric_by_class = MulticlassAccuracy(num_classes=model.output_size, average=None)\n",
    "cat_metric_by_class.update(torch.cat(y_preds), torch.cat(y_truths))\n",
    "_, ax = cat_metric_by_class.plot()\n",
    "ax.legend(bbox_to_anchor=(1.1, 1.05))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34bb699e",
   "metadata": {},
   "source": [
    "We can also get a more qualitative assessment by comparing some of the original and reconstructed images. \n",
    "- The examples are all correctly characterised\n",
    "- The reconstructed images are clearly of the correct character. However, the reconstructed images are noticeably blurrier, which is a common phenomenon with VAEs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8712f78f",
   "metadata": {
    "id": "OjO1z_Pswd4M",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "\n",
    "fig = plt.figure(figsize=(8.0, 10.0))\n",
    "grid = ImageGrid(fig, 111, nrows_ncols=(2, 10), axes_pad=(0.05, 0.5))\n",
    "\n",
    "for ax, im, y_true in zip(grid[:10], x_truths[0], y_truths[0]):\n",
    "    ax.imshow(im.squeeze(), cmap='gray')\n",
    "    ax.set_title(label_to_string(y_true.numpy()))\n",
    "grid[0].set_ylabel(\"Truth\")\n",
    "\n",
    "for ax, im, y_pred in zip(grid[10:], x_recons[0], y_preds[0]):\n",
    "    ax.imshow(im.squeeze(), cmap='gray')\n",
    "    ax.set_title(label_to_string(y_pred.numpy()))\n",
    "grid[10].set_ylabel(\"Gen.\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8308ff9d",
   "metadata": {
    "id": "dpuwrqfkwd4M",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# Generation of synthetic hand-writing\n",
    "We can now use the VAE to generate some synthetic handwriting.\n",
    "- Use the encoder to determine the hand-writing style (in terms of latent variables) from the first 6 items in the test set\n",
    "- Use the decoder to generate the full set of characters for each of these styles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1049da",
   "metadata": {
    "id": "6-vo2ULmnLlV",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "labels = torch.arange(0, model.output_size)\n",
    "y_plot = nn.functional.one_hot(labels.reshape(-1, 1).tile(1, 6).flatten()).float().to(device)\n",
    "\n",
    "z_mean_to_plot = z_means[0][:6].to(device)\n",
    "log_z_var_to_plot = log_z_vars[0][:6].to(device)\n",
    "z_plot = model.sample(z_mean_to_plot, log_z_var_to_plot).tile(model.output_size, 1)\n",
    "generated_images = model.decode(z_plot, y_plot).sigmoid().reshape(-1, model.input_size, model.input_size).cpu().detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3668eaf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15.0, 10.0))\n",
    "grid = ImageGrid(fig, 121, nrows_ncols=(model.output_size, 6), axes_pad=0.05)\n",
    "\n",
    "for ax, im in zip(grid, generated_images):\n",
    "    ax.imshow(im, cmap='gray')\n",
    "\n",
    "for i, label in enumerate(labels):\n",
    "    grid[i * 6].set_ylabel(label_to_string(label))\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "M2 model.ipynb",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 9726,
     "sourceId": 17999,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30587,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 846.102374,
   "end_time": "2023-12-01T12:30:02.361288",
   "environment_variables": {},
   "exception": true,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-12-01T12:15:56.258914",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
